chaperone - 1 per ferret's machine.  accept "git pull" requests.  clones data in new dir, stops ferret. restarts ferret in cloned data, then updates old data.  actually, no, just delete the old data!

write out complete v3 design here.  all components.  species, data structures etc.


*** ####################################   DynamoDB Tables   ####################################### ***

ğŸ—ƒğŸ§¨ REPOS_TABLE
        repoName, indexName, shardName, gitUsername,
            - for assigning repos to shards
            - for deleting indexes, freeing repos
            - repoName is a shortened url, like "github.com/wdbvctestorg/0"
            - shardId examples: "indie3:2", "na"
            - gitUsername - store this cos diff data could be in diff git accounts, providers
                - where is password stored?
ğŸ‰ğŸ§¨ FERRETS_TABLE
        ipAddress, port, status, cloudProvider, cloudRegion, indexName, shardName
            - to fetch all ferret for a specific index
                - later, just get nearby ferrets?
                - fetched by Oracle.  (at most 1 per region)
            - written to by Chaperone and Overmind(?)
                - shit, previously, ferrets were supposed to self-report ...
                - who starts/stops ferrets?? overmind?? how many???
            - status: startingUp, active, spotShuttingDown
ğŸ‘©â€ğŸ¦³ğŸ§¨ CHAPERONES_TABLE
        ipAddress, port, status, cloudProvider, cloudRegion, <machineStats1>, <machineStats2>
ğŸ‘©â€ğŸ«ğŸ§¨ LIBRARIANS_TABLE
        ipAddress, port, indexName, status?
        1 or more librarians per library =spotEC2
ğŸ‘©â€ğŸ’¼ğŸ§¨ LIBRARYMANAGERS_TABLE
        ipAddress, port, status, cloudProvider, cloudRegion, <machineStats1>, <machineStats2>
        1 libraryManager per library = spotEC2
ğŸ”®ğŸ§¨ ORACLES_TABLE
        ipAddress, port, status, cloudProvider, cloudRegion, <machineStats1>, <machineStats2>




^^ should these be combined?? endpoints table>?
ğŸ’ğŸ§¨ MISC_TABLE      -    holds dispatch
        key
        keys: dispatchEndpoint
        jobIds? statuses?  a job ID created per index change request (add doc, create index ect)
ğŸ“˜ğŸ§¨ INDEXES_TABLE
        indexName, customerId, schema, status, settings, size <other>
        status: index could be archived? or active?
        settings could be like... auto-global deploy?  dynamic deployment vs always-on ?
        size = total size of all shards combined.  Capitol uses this when finding a library to put this index when needed.

ğŸ“–ğŸ§¨ INDEX_UPDATES_TABLE
        jobID, status
        does NOT store entire request.  request just sits in the Capitol until a librarian is ready.  Capitol is NOT spot instance. permanent box.

*** #######################################    Machines    ########################################## ***

ğŸ£ NEST
    a nest of ferrets and 1 chaperone
    usually referred to as "chaperone" ğŸ‘©â€ğŸ¦³ (cos only 1 ğŸ‘©â€ğŸ¦³ per ğŸ£)
    spotEC2

ğŸ° Capitol
    1 of these, permanent - houses the ğŸ‘©â€ğŸ’» dispatch and/or ğŸ§  overmind
    single machine or load balanced identical machines
    --- i think this should be a bunch of lambdas! not a machine!
    --- actually just a single lambda? to minimize cold starts
    ---  no, more than 1. cos some public, some not.

ğŸ› Delphi
    spotEC2 - houses a single ğŸ”®
    minimum 1 per region (that contains a nest)

ğŸ“š LIBRARY
    spotEC2 of 1 LibraryManager ğŸ‘©â€ğŸ« and 1 or more librarians
    1 librarian per index
    LibraryManager starts and stops librarians on his machine
    libraries can be turned off when not needed
    spotEC2

*** #######################################    Species    ########################################## ***
------------------------------------------------------------------------------------------------------
ğŸ‰ FERRET
    1 shard per ferret
    responds to queries from Oracle.
    writes local logs for Chaperone monitor
    how does it stop?  Chaperone, only.  (starts/stops for data update.  stops for spotEC2 shutdown. stops per manual request)
    *** DISTRIBUTION ***
    several ferrets per ferret spotEC2
    *** POPULATION ***
    many ferret spotEC2, so many ferrets
    *** STARTUP ***
    - sets self Status to Active in FERRETS_TABLE at first startup (chaperone set cmd line arg for startup vs pull-restart)
    *** DDB-VISIBILITY ***
    - FERRETS_TABLE
    *** STATE ***
    - db - the rocksdb object
         - created at startup and passed to server client
         - dir passed as cmd line arg by Chaperone
            - or... should dir be a uuid? does it matter what it's named?
    *** ENDPOINTS ***
    - query(splitTokens, options/settigns(?))
------------------------------------------------------------------------------------------------------
ğŸ‘©â€ğŸ¦³ CHAPERONE
    auto-starts with new spotEC2 instance - starts/stops/updates ferrets - updates shards
    updates Dispatch when
    *** DISTRIBUTION ***
    1 chaperone per ferret spotEC2
    *** POPULATION ***
    many ferret spotEC2, so many ferret chaperones
    *** STARTUP ***
    spins up monitor process -- alerts Overmind if overcapacity -- manages local ferret logs to determine which Index is overworked
    *** SPOT SHUTDOWN ***
    - sets all ferrets in FERRETS_TABLE to status=spotShuttingDown
    - who to tell?  how to ensure new replacement ferrets started? i think that's what overmind is for? also update dispatch?
    *** DDB-VISIBILITY ***
    - FERRET_TABLE
    *** STATE ***
    - root dir path (1st children: index root dirs)
    - ip address
    - cloudProvider
    - cloudRegion
    - the Overmind endpoint
    - the Dispatch endpoint
    *** ENDPOINTS ** QUEUED INSTRUCTIONS ***
    - startFerret(indexName, shardName)
        - clones shard repo, adds ferret with status=startingUp FERRETS_TABLE
    - pullShard(indexName, shardName)
        - downloads a 2nd clone of the shard.  switches ferret to new shard, deletes old shard.  downtime should be milliseconds.  oracle retries if fail.  no change to FERRETS_TABLE
    - pullIndex(indexName) - updates all ferrets for that index.  iterates through child dirs. (is this necessary?)
------------------------------------------------------------------------------------------------------
ğŸ‘©â€ğŸ« LIBRARIAN
    librarian must act in a single process (per index)
    take commands from a queue
    *** DISTRIBUTION ***
    1 ğŸ‘©â€ğŸ« per ğŸ“š(index)
    *** POPULATION ***
    multiple ğŸ™‹â€â™€ï¸ per ğŸ›
    *** DDB-VISIBILITY ***
    ğŸ‘©â€ğŸ«ğŸ§¨ LIBRARIANS_TABLE
    ğŸ“˜ğŸ§¨ INDEXES_TABLE - librarian updates index size here after adding a new doc and git-pushing
    *** STATE ***
    - index root dir
    - map<shardName, (gitUrl, gitUsername, gitPassword)> (passed as cli args by librarymanager)
    *** ENDPOINTS ** QUEUED INSTRUCTIONS ***
    - createIndex(indexName, schema)
    - getIndexSchema(indexName)
    - getIndexStats(indexName)
    - deleteIndex(indexName)
    - indexDocument(doc, indexName)
        - if need new shard, get unused repo from and update REPOS_TABLE
        - cannot be in parallel
    - deleteDocument(docId, indexName)
    - query(splitTokens, options/settigns(?)) --- for testing.  uses fresh db objects (so not a performance test)
------------------------------------------------------------------------------------------------------

this is outdated!  wrong!  idk what's right! libraryadmin DNE anymore! - sept8 2020

ğŸ‘©â€ğŸ’¼ LIBRARYMANAGER
    starts and stops ğŸ‘©â€ğŸ« (librarians)
    why? so multiple librarians can share a spot EC2 instance
    dispatch accepts indexChange requests and sends to appropriate librarian,
    monitors libraries' sizes, tells libraryadmin when overcapacity.
    *** DISTRIBUTION ***
    1 ğŸ‘©â€ğŸ« per ğŸ›
    *** POPULATION ***
    can be multiple ğŸ™‹â€â™€ï¸ per ğŸ›
    *** DDB-VISIBILITY ***
    ğŸ‘©â€ğŸ«ğŸ§¨ LIBRARYMANAGERS_TABLE -- tell it when itself is active
    ğŸ—ƒğŸ§¨ REPOS_TABLE - to give repos info and shards info to librarians
    *** STATE ***
    indexes root dir
    libraryadmin endpoint
    *** ENDPOINTS ***
    startLibrarian(indexName)
------------------------------------------------------------------------------------------------------
ğŸ”® ORACLE
    receives query request directly from web browser. distributes among ferrets and returns the first page of results
    *** DISTRIBUTION ***
    1 or more Oracles per index per region.
    *** POPULATION ***
    at least 1 oracle per index.
    *** DDB-VISIBILITY ***
    *** STATE ***
    set by the Capitol
    - map<indexName, listOfAllShards>
    - map<shardId, ferret_endpoints> (1 or more ferret endpoint per shardId)
    *** ENDPOINTS ***
    - query(splitTokens, options/settigns(?))
------------------------------------------------------------------------------------------------------

should these all be lambdas??????

ğŸ‘©â€ğŸ’» DISPATCH - rename to overmind ğŸ§  ? -- or ... 1 port endpoint at the Capitol ğŸ°
    2 types of tasks
    - ensure resources are under capacity
        - zergforce, oracles, librarians,
    - accept and process public index change requetts (pull, create, delete, etc)
        - store these in ddb (always? just when librarian not available yet?)
    do all non-immediate actions across zergforces (everything besides querying)
    - keep track of where all ferrets, chaperones, and oracles are
    - get status change notifications from ferrets, chaperones, and oracles
    - issue pull commands to zergforces per library update
    - keeps local in-memory clone of FERRETS_TABLE up to date
    - receives at-capacity and over-capacity messages from Oracles and Chaperones (with offending Index info)
    - ensures enough Oracles and indexzergforce.
    - spawns/rebalances/kills Oracles, Chaperones, and Ferret distribution according to usage demand
    - issue ferret-kill command to Chaperone
    - does NOT send messages to ferrets.  commands Ferrets via Chaperone
    - do role of library-administrator.  rebalance librarians among the libraries as indexes shrink and grow.
    - also, turn off librarians at night etc, when able to.  when librarian is off, index change requests get stored in ddb until librarian is ready for them.
    *** DISTRIBUTION ***
    just 1 at global HQ (virginia?)
    *** POPULATION ***
    just 1
    *** DDB-VISIBILITY ***
    FERRETS_TABLE
    *** STATE ***
    FERRETS_TABLE local in-memory clone
    all chaperone, ferrets, oracle locations
    *** ENDPOINTS ***
    - gitPullIndex
    - gitPullShard
    - gitPullEverything
    - nestsHealthCheck (a nest is a collection of ferrets under a single Chaperone, on a single spotEC2)
    - shardsHealthCheck
    - destroyIndex
    - destroyEverything
------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------
***-------------------------------------- ***  F U T U R E  *** -----------------------------------***
------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------

ğŸ”­ SCOPE
    this is the machine that serves the website that provides a real-time view of current searches around the world
    when someone is on the scope website, all the oracles get told to send a copy of their search activity to the worpscope.  the scope then organizes these into a nice dynamic UI map